{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6b4af694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b29e2f",
   "metadata": {},
   "source": [
    "seed 값 줘야지 돌릴때마다 랜덤 w, b 값이 변형이 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202bae45",
   "metadata": {},
   "source": [
    "5행 1열로 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6ef44e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]])\n",
    "y_train = torch.tensor([[3.0], [5.0], [7.0], [9.0], [11.0]])\n",
    "print(x_train.size())\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5c9bb915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W=torch.zeros(1, requires_grad=True)\n",
    "b=torch.zeros(1, requires_grad=True)\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651192c",
   "metadata": {},
   "source": [
    "수식 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f310c1",
   "metadata": {},
   "source": [
    "랜덤하게 만든게 아니고 0값을 줬으니까 아래의 결과가 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0f2df450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y=x_train*W+b\n",
    "print('y=',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa9975",
   "metadata": {},
   "source": [
    "cost 함수 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e113db",
   "metadata": {},
   "source": [
    "y_train = torch.tensor([[3.0], [5.0], [7.0], [9.0], [11.0]]) 제곱한걸 다 더해서 평균 낸 것\n",
    "손실함수야."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4dc4a19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(57., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost=torch.mean((y-y_train)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e68073",
   "metadata": {},
   "source": [
    "최적화 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aed70609",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W,b], lr=0.01) #e-2과 같아\n",
    "epochs=2001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3b100",
   "metadata": {},
   "source": [
    "2x+1 의 식을 갖어.\n",
    "지금은 우리가 입력 값이 작아서 베치사이즈를 안준거야.\n",
    "베치사이를 준다면 for문으로 써. 100을 준다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "66feda78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0,Loss:57.0, w:0.5, b:0.14000000059604645\n",
      "Epoch10,Loss:0.28650277853012085, w:2.002049446105957, b:0.5708461999893188\n",
      "Epoch20,Loss:0.02843988500535488, w:2.099571943283081, b:0.6121681332588196\n",
      "Epoch30,Loss:0.0254964642226696, w:2.1028151512145996, b:0.6268995404243469\n",
      "Epoch40,Loss:0.023821769282221794, w:2.09983229637146, b:0.6394458413124084\n",
      "Epoch50,Loss:0.02226170152425766, w:2.0965375900268555, b:0.6514604091644287\n",
      "Epoch60,Loss:0.020803730934858322, w:2.093324661254883, b:0.6630672216415405\n",
      "Epoch70,Loss:0.01944129168987274, w:2.090217113494873, b:0.6742870807647705\n",
      "Epoch80,Loss:0.01816803589463234, w:2.0872130393981934, b:0.6851333975791931\n",
      "Epoch90,Loss:0.016978196799755096, w:2.084308624267578, b:0.6956184506416321\n",
      "Epoch100,Loss:0.01586623676121235, w:2.0815012454986572, b:0.7057543396949768\n",
      "Epoch110,Loss:0.01482720859348774, w:2.078787326812744, b:0.7155526876449585\n",
      "Epoch120,Loss:0.013856078498065472, w:2.0761637687683105, b:0.7250247597694397\n",
      "Epoch130,Loss:0.012948649935424328, w:2.073627471923828, b:0.7341814041137695\n",
      "Epoch140,Loss:0.012100661173462868, w:2.0711755752563477, b:0.7430331110954285\n",
      "Epoch150,Loss:0.011308213695883751, w:2.068805694580078, b:0.7515900731086731\n",
      "Epoch160,Loss:0.01056760922074318, w:2.066514492034912, b:0.7598620057106018\n",
      "Epoch170,Loss:0.009875544346868992, w:2.0642995834350586, b:0.7678584456443787\n",
      "Epoch180,Loss:0.009228763170540333, w:2.0621583461761475, b:0.7755887508392334\n",
      "Epoch190,Loss:0.008624388836324215, w:2.0600881576538086, b:0.7830615639686584\n",
      "Epoch200,Loss:0.008059575222432613, w:2.0580873489379883, b:0.7902857065200806\n",
      "Epoch210,Loss:0.00753166526556015, w:2.0561530590057373, b:0.7972694039344788\n",
      "Epoch220,Loss:0.007038424257189035, w:2.0542831420898438, b:0.804020345211029\n",
      "Epoch230,Loss:0.006577492691576481, w:2.0524754524230957, b:0.8105465173721313\n",
      "Epoch240,Loss:0.006146705709397793, w:2.0507280826568604, b:0.8168553709983826\n",
      "Epoch250,Loss:0.005744181107729673, w:2.049038887023926, b:0.822953999042511\n",
      "Epoch260,Loss:0.005367990117520094, w:2.047405958175659, b:0.8288496136665344\n",
      "Epoch270,Loss:0.005016390234231949, w:2.0458273887634277, b:0.8345488905906677\n",
      "Epoch280,Loss:0.0046879020519554615, w:2.0443015098571777, b:0.8400583267211914\n",
      "Epoch290,Loss:0.0043809073977172375, w:2.0428261756896973, b:0.8453842401504517\n",
      "Epoch300,Loss:0.004093998111784458, w:2.04140043258667, b:0.8505327105522156\n",
      "Epoch310,Loss:0.003825885709375143, w:2.0400214195251465, b:0.8555097579956055\n",
      "Epoch320,Loss:0.003575321286916733, w:2.038688898086548, b:0.8603212833404541\n",
      "Epoch330,Loss:0.0033411618787795305, w:2.037400484085083, b:0.8649725317955017\n",
      "Epoch340,Loss:0.0031223592814058065, w:2.0361549854278564, b:0.8694689273834229\n",
      "Epoch350,Loss:0.002917868783697486, w:2.0349509716033936, b:0.8738155961036682\n",
      "Epoch360,Loss:0.002726761857047677, w:2.0337870121002197, b:0.8780176043510437\n",
      "Epoch370,Loss:0.0025481977500021458, w:2.0326621532440186, b:0.8820794820785522\n",
      "Epoch380,Loss:0.0023812996223568916, w:2.0315744876861572, b:0.8860061764717102\n",
      "Epoch390,Loss:0.002225356176495552, w:2.0305230617523193, b:0.8898021578788757\n",
      "Epoch400,Loss:0.0020796258468180895, w:2.0295066833496094, b:0.8934717774391174\n",
      "Epoch410,Loss:0.0019434156129136682, w:2.028524160385132, b:0.89701908826828\n",
      "Epoch420,Loss:0.0018161498010158539, w:2.027574300765991, b:0.9004482626914978\n",
      "Epoch430,Loss:0.0016972168814390898, w:2.026656150817871, b:0.9037632942199707\n",
      "Epoch440,Loss:0.00158606912009418, w:2.025768280029297, b:0.9069679379463196\n",
      "Epoch450,Loss:0.001482193125411868, w:2.0249104499816895, b:0.9100658297538757\n",
      "Epoch460,Loss:0.0013851090334355831, w:2.024080753326416, b:0.9130606651306152\n",
      "Epoch470,Loss:0.001294414047151804, w:2.0232791900634766, b:0.915955662727356\n",
      "Epoch480,Loss:0.0012096459977328777, w:2.0225038528442383, b:0.9187542796134949\n",
      "Epoch490,Loss:0.001130417687818408, w:2.021754503250122, b:0.9214596748352051\n",
      "Epoch500,Loss:0.00105638790410012, w:2.0210301876068115, b:0.9240749478340149\n",
      "Epoch510,Loss:0.0009872224181890488, w:2.0203299522399902, b:0.9266030192375183\n",
      "Epoch520,Loss:0.0009225585381500423, w:2.019652843475342, b:0.9290470480918884\n",
      "Epoch530,Loss:0.0008621446904726326, w:2.018998384475708, b:0.9314098954200745\n",
      "Epoch540,Loss:0.0008056759834289551, w:2.0183656215667725, b:0.9336940050125122\n",
      "Epoch550,Loss:0.000752905965782702, w:2.017754077911377, b:0.9359020590782166\n",
      "Epoch560,Loss:0.0007036016322672367, w:2.017162799835205, b:0.9380366206169128\n",
      "Epoch570,Loss:0.0006575107108801603, w:2.0165913105010986, b:0.9401000142097473\n",
      "Epoch580,Loss:0.0006144570070318878, w:2.016038656234741, b:0.9420947432518005\n",
      "Epoch590,Loss:0.0005742132198065519, w:2.0155045986175537, b:0.9440230131149292\n",
      "Epoch600,Loss:0.0005366100231185555, w:2.014988422393799, b:0.9458870887756348\n",
      "Epoch610,Loss:0.0005014581838622689, w:2.01448917388916, b:0.9476891160011292\n",
      "Epoch620,Loss:0.00046862693852744997, w:2.0140068531036377, b:0.9494310021400452\n",
      "Epoch630,Loss:0.0004379269084893167, w:2.013540267944336, b:0.9511148929595947\n",
      "Epoch640,Loss:0.000409252563258633, w:2.013089418411255, b:0.9527428150177002\n",
      "Epoch650,Loss:0.0003824445593636483, w:2.0126535892486572, b:0.9543166756629944\n",
      "Epoch660,Loss:0.0003574038273654878, w:2.0122320652008057, b:0.9558380246162415\n",
      "Epoch670,Loss:0.0003339886025059968, w:2.0118248462677, b:0.9573086500167847\n",
      "Epoch680,Loss:0.00031211692839860916, w:2.0114309787750244, b:0.9587302803993225\n",
      "Epoch690,Loss:0.00029167960747145116, w:2.0110504627227783, b:0.9601045250892639\n",
      "Epoch700,Loss:0.00027257041074335575, w:2.0106823444366455, b:0.9614329934120178\n",
      "Epoch710,Loss:0.00025472865672782063, w:2.010326623916626, b:0.9627172946929932\n",
      "Epoch720,Loss:0.0002380406658630818, w:2.0099828243255615, b:0.963958740234375\n",
      "Epoch730,Loss:0.00022245911532081664, w:2.009650468826294, b:0.965158998966217\n",
      "Epoch740,Loss:0.00020788318943232298, w:2.009329080581665, b:0.96631920337677\n",
      "Epoch750,Loss:0.00019426390645094216, w:2.0090184211730957, b:0.9674407243728638\n",
      "Epoch760,Loss:0.00018154439749196172, w:2.0087180137634277, b:0.9685249924659729\n",
      "Epoch770,Loss:0.0001696583058219403, w:2.008427858352661, b:0.9695730209350586\n",
      "Epoch780,Loss:0.00015854761295486242, w:2.0081470012664795, b:0.9705862998962402\n",
      "Epoch790,Loss:0.0001481568324379623, w:2.007875919342041, b:0.9715657830238342\n",
      "Epoch800,Loss:0.00013846071669831872, w:2.0076136589050293, b:0.9725126028060913\n",
      "Epoch810,Loss:0.00012939475709572434, w:2.0073599815368652, b:0.9734278917312622\n",
      "Epoch820,Loss:0.00012092195538571104, w:2.007114887237549, b:0.9743127226829529\n",
      "Epoch830,Loss:0.00011299954348942265, w:2.006877899169922, b:0.9751680493354797\n",
      "Epoch840,Loss:0.00010560268856352195, w:2.0066487789154053, b:0.9759950041770935\n",
      "Epoch850,Loss:9.868356573861092e-05, w:2.006427526473999, b:0.976794421672821\n",
      "Epoch860,Loss:9.222463995683938e-05, w:2.006213426589966, b:0.9775671362876892\n",
      "Epoch870,Loss:8.618256106274202e-05, w:2.0060064792633057, b:0.9783141613006592\n",
      "Epoch880,Loss:8.053674537222832e-05, w:2.0058066844940186, b:0.9790362119674683\n",
      "Epoch890,Loss:7.526491390308365e-05, w:2.005613327026367, b:0.9797341823577881\n",
      "Epoch900,Loss:7.033418660284951e-05, w:2.0054264068603516, b:0.9804090857505798\n",
      "Epoch910,Loss:6.572627899004146e-05, w:2.0052456855773926, b:0.9810614585876465\n",
      "Epoch920,Loss:6.142730853753164e-05, w:2.0050711631774902, b:0.9816920161247253\n",
      "Epoch930,Loss:5.740196866099723e-05, w:2.0049021244049072, b:0.9823015928268433\n",
      "Epoch940,Loss:5.364221942727454e-05, w:2.0047390460968018, b:0.9828909039497375\n",
      "Epoch950,Loss:5.012726251152344e-05, w:2.0045812129974365, b:0.9834606647491455\n",
      "Epoch960,Loss:4.6849359932821244e-05, w:2.0044288635253906, b:0.9840112924575806\n",
      "Epoch970,Loss:4.37776543549262e-05, w:2.0042812824249268, b:0.9845437407493591\n",
      "Epoch980,Loss:4.091209848411381e-05, w:2.004138708114624, b:0.9850584268569946\n",
      "Epoch990,Loss:3.8233465602388605e-05, w:2.0040009021759033, b:0.9855560064315796\n",
      "Epoch1000,Loss:3.572796413209289e-05, w:2.0038676261901855, b:0.9860368967056274\n",
      "Epoch1010,Loss:3.338949318276718e-05, w:2.0037388801574707, b:0.9865018129348755\n",
      "Epoch1020,Loss:3.120335895800963e-05, w:2.0036144256591797, b:0.9869512915611267\n",
      "Epoch1030,Loss:2.9159366022213362e-05, w:2.0034940242767334, b:0.9873856902122498\n",
      "Epoch1040,Loss:2.72507022600621e-05, w:2.003377676010132, b:0.9878056645393372\n",
      "Epoch1050,Loss:2.546472205722239e-05, w:2.003265142440796, b:0.9882116913795471\n",
      "Epoch1060,Loss:2.3798575057298876e-05, w:2.0031564235687256, b:0.9886042475700378\n",
      "Epoch1070,Loss:2.2237387383938767e-05, w:2.003051519393921, b:0.988983690738678\n",
      "Epoch1080,Loss:2.078434408758767e-05, w:2.0029499530792236, b:0.9893503189086914\n",
      "Epoch1090,Loss:1.9421464457991533e-05, w:2.0028514862060547, b:0.989704966545105\n",
      "Epoch1100,Loss:1.8149969037040137e-05, w:2.0027565956115723, b:0.9900477528572083\n",
      "Epoch1110,Loss:1.6963196685537696e-05, w:2.002664804458618, b:0.9903792142868042\n",
      "Epoch1120,Loss:1.585143581905868e-05, w:2.0025761127471924, b:0.9906995296478271\n",
      "Epoch1130,Loss:1.4813695088378154e-05, w:2.002490282058716, b:0.9910092353820801\n",
      "Epoch1140,Loss:1.3842369298799895e-05, w:2.0024075508117676, b:0.9913085699081421\n",
      "Epoch1150,Loss:1.2937716746819206e-05, w:2.0023274421691895, b:0.9915978908538818\n",
      "Epoch1160,Loss:1.208946923725307e-05, w:2.0022499561309814, b:0.9918776154518127\n",
      "Epoch1170,Loss:1.1298527169856243e-05, w:2.0021750926971436, b:0.9921480417251587\n",
      "Epoch1180,Loss:1.0558707799646072e-05, w:2.0021026134490967, b:0.9924094676971436\n",
      "Epoch1190,Loss:9.867613698588684e-06, w:2.002032518386841, b:0.9926621913909912\n",
      "Epoch1200,Loss:9.221222171618138e-06, w:2.001964807510376, b:0.9929065108299255\n",
      "Epoch1210,Loss:8.616650120529812e-06, w:2.001899480819702, b:0.9931426644325256\n",
      "Epoch1220,Loss:8.052140401559882e-06, w:2.0018362998962402, b:0.9933709502220154\n",
      "Epoch1230,Loss:7.52688356442377e-06, w:2.001775026321411, b:0.9935917258262634\n",
      "Epoch1240,Loss:7.032192570477491e-06, w:2.001715898513794, b:0.9938051104545593\n",
      "Epoch1250,Loss:6.572193342435639e-06, w:2.0016586780548096, b:0.9940112829208374\n",
      "Epoch1260,Loss:6.1418932091328315e-06, w:2.001603603363037, b:0.9942106604576111\n",
      "Epoch1270,Loss:5.740489541494753e-06, w:2.0015504360198975, b:0.9944033026695251\n",
      "Epoch1280,Loss:5.364960998122115e-06, w:2.0014986991882324, b:0.9945895671844482\n",
      "Epoch1290,Loss:5.012533165427158e-06, w:2.001448631286621, b:0.994769811630249\n",
      "Epoch1300,Loss:4.684703071689e-06, w:2.0014007091522217, b:0.9949439167976379\n",
      "Epoch1310,Loss:4.378010089567397e-06, w:2.0013539791107178, b:0.9951122999191284\n",
      "Epoch1320,Loss:4.0919876482803375e-06, w:2.0013086795806885, b:0.9952749013900757\n",
      "Epoch1330,Loss:3.824140549113508e-06, w:2.001265525817871, b:0.9954321384429932\n",
      "Epoch1340,Loss:3.573371259335545e-06, w:2.001223087310791, b:0.9955841302871704\n",
      "Epoch1350,Loss:3.3395619993825676e-06, w:2.0011825561523438, b:0.9957311749458313\n",
      "Epoch1360,Loss:3.1209588087222073e-06, w:2.001143217086792, b:0.9958732724189758\n",
      "Epoch1370,Loss:2.9173475013521966e-06, w:2.0011050701141357, b:0.9960105419158936\n",
      "Epoch1380,Loss:2.725351350818528e-06, w:2.001068353652954, b:0.9961433410644531\n",
      "Epoch1390,Loss:2.5464441932854243e-06, w:2.001032590866089, b:0.9962717294692993\n",
      "Epoch1400,Loss:2.380191745032789e-06, w:2.0009984970092773, b:0.9963959455490112\n",
      "Epoch1410,Loss:2.2248768800636753e-06, w:2.000965118408203, b:0.9965158104896545\n",
      "Epoch1420,Loss:2.0789154859812697e-06, w:2.0009331703186035, b:0.9966318607330322\n",
      "Epoch1430,Loss:1.942759354278678e-06, w:2.0009021759033203, b:0.9967438578605652\n",
      "Epoch1440,Loss:1.8154898953071097e-06, w:2.0008718967437744, b:0.9968522191047668\n",
      "Epoch1450,Loss:1.6967539977486013e-06, w:2.0008432865142822, b:0.9969568848609924\n",
      "Epoch1460,Loss:1.5861394331295742e-06, w:2.00081467628479, b:0.997058093547821\n",
      "Epoch1470,Loss:1.4822246612311574e-06, w:2.0007879734039307, b:0.997156023979187\n",
      "Epoch1480,Loss:1.3855702718501561e-06, w:2.0007617473602295, b:0.997250497341156\n",
      "Epoch1490,Loss:1.2946563856530702e-06, w:2.0007362365722656, b:0.997342050075531\n",
      "Epoch1500,Loss:1.2102291293558665e-06, w:2.0007119178771973, b:0.9974305629730225\n",
      "Epoch1510,Loss:1.130985765485093e-06, w:2.000688076019287, b:0.9975160360336304\n",
      "Epoch1520,Loss:1.0568774087005295e-06, w:2.0006649494171143, b:0.9975988268852234\n",
      "Epoch1530,Loss:9.878306173050078e-07, w:2.000643253326416, b:0.9976786971092224\n",
      "Epoch1540,Loss:9.228906492353417e-07, w:2.000621795654297, b:0.997755765914917\n",
      "Epoch1550,Loss:8.622890277365514e-07, w:2.000601053237915, b:0.9978305101394653\n",
      "Epoch1560,Loss:8.058262892518542e-07, w:2.0005810260772705, b:0.9979026317596436\n",
      "Epoch1570,Loss:7.535269332947792e-07, w:2.0005619525909424, b:0.9979723691940308\n",
      "Epoch1580,Loss:7.042842753435252e-07, w:2.0005428791046143, b:0.9980397820472717\n",
      "Epoch1590,Loss:6.578059128514724e-07, w:2.0005249977111816, b:0.9981050491333008\n",
      "Epoch1600,Loss:6.152300215944706e-07, w:2.0005075931549072, b:0.9981681108474731\n",
      "Epoch1610,Loss:5.749653269049304e-07, w:2.00049090385437, b:0.998228907585144\n",
      "Epoch1620,Loss:5.374203624342044e-07, w:2.000474214553833, b:0.9982877969741821\n",
      "Epoch1630,Loss:5.020018534196424e-07, w:2.0004584789276123, b:0.998344898223877\n",
      "Epoch1640,Loss:4.6924202479203814e-07, w:2.00044322013855, b:0.9984000325202942\n",
      "Epoch1650,Loss:4.386612602047535e-07, w:2.0004286766052246, b:0.9984531998634338\n",
      "Epoch1660,Loss:4.10034544984228e-07, w:2.0004143714904785, b:0.9985045194625854\n",
      "Epoch1670,Loss:3.8293910620268434e-07, w:2.0004003047943115, b:0.9985544085502625\n",
      "Epoch1680,Loss:3.5787633123618434e-07, w:2.000386953353882, b:0.998602569103241\n",
      "Epoch1690,Loss:3.34616288455436e-07, w:2.0003743171691895, b:0.9986491203308105\n",
      "Epoch1700,Loss:3.1270673161998275e-07, w:2.000361919403076, b:0.9986939430236816\n",
      "Epoch1710,Loss:2.920162387454184e-07, w:2.000349998474121, b:0.9987373352050781\n",
      "Epoch1720,Loss:2.73229204594827e-07, w:2.000338077545166, b:0.9987792372703552\n",
      "Epoch1730,Loss:2.5531721803417895e-07, w:2.000326633453369, b:0.9988200068473816\n",
      "Epoch1740,Loss:2.383096671110252e-07, w:2.0003159046173096, b:0.9988594055175781\n",
      "Epoch1750,Loss:2.2285739476046729e-07, w:2.000305414199829, b:0.9988973736763\n",
      "Epoch1760,Loss:2.0825541469093878e-07, w:2.000295400619507, b:0.9989339709281921\n",
      "Epoch1770,Loss:1.948902053072743e-07, w:2.0002858638763428, b:0.9989692568778992\n",
      "Epoch1780,Loss:1.821278203806287e-07, w:2.0002763271331787, b:0.9990034103393555\n",
      "Epoch1790,Loss:1.7010424357977172e-07, w:2.0002667903900146, b:0.9990364909172058\n",
      "Epoch1800,Loss:1.5885879633970035e-07, w:2.000257968902588, b:0.9990686774253845\n",
      "Epoch1810,Loss:1.4847159945929889e-07, w:2.0002493858337402, b:0.9990997314453125\n",
      "Epoch1820,Loss:1.3878675986234157e-07, w:2.0002410411834717, b:0.9991297125816345\n",
      "Epoch1830,Loss:1.29680898908191e-07, w:2.0002331733703613, b:0.9991586804389954\n",
      "Epoch1840,Loss:1.2121191161895695e-07, w:2.00022554397583, b:0.9991865754127502\n",
      "Epoch1850,Loss:1.1351207973575583e-07, w:2.000218152999878, b:0.9992135167121887\n",
      "Epoch1860,Loss:1.0597796062938869e-07, w:2.000211000442505, b:0.9992395043373108\n",
      "Epoch1870,Loss:9.898955255494002e-08, w:2.000203847885132, b:0.999264657497406\n",
      "Epoch1880,Loss:9.261111699743196e-08, w:2.000196695327759, b:0.9992891550064087\n",
      "Epoch1890,Loss:8.651932148495689e-08, w:2.000190258026123, b:0.9993128776550293\n",
      "Epoch1900,Loss:8.088132830152972e-08, w:2.0001838207244873, b:0.9993358254432678\n",
      "Epoch1910,Loss:7.556604941783007e-08, w:2.0001778602600098, b:0.9993579983711243\n",
      "Epoch1920,Loss:7.061576923206303e-08, w:2.0001718997955322, b:0.9993792772293091\n",
      "Epoch1930,Loss:6.607911018363666e-08, w:2.000166177749634, b:0.9993999004364014\n",
      "Epoch1940,Loss:6.169216248963494e-08, w:2.0001609325408936, b:0.9994198679924011\n",
      "Epoch1950,Loss:5.771207867155681e-08, w:2.000155448913574, b:0.9994390606880188\n",
      "Epoch1960,Loss:5.384872636682303e-08, w:2.000150442123413, b:0.9994576573371887\n",
      "Epoch1970,Loss:5.035112593532176e-08, w:2.000145673751831, b:0.9994755387306213\n",
      "Epoch1980,Loss:4.7101185174369675e-08, w:2.000140905380249, b:0.9994927048683167\n",
      "Epoch1990,Loss:4.410035359114772e-08, w:2.000136137008667, b:0.9995093941688538\n",
      "Epoch2000,Loss:4.1229803571241064e-08, w:2.000131368637085, b:0.9995256066322327\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "  #베치사이즈 만큼 x데이터 y데이터 만큼 불러서 아래의 optimizer ~ 부터 한번 하고 그 아래에 하나씩 하는 것\n",
    "  # 1에폭이 100만개, 배치가 100개면 1만번 돌면 1에폭\n",
    "\n",
    "  optimizer.zero_grad() #한번 돌면 optim 값이 생기니까 클리어\n",
    "\n",
    "  y= x_train*W + b #가상의 값 만들기\n",
    "  cost=torch.mean((y-y_train)**2) #제곱한거의 평균 , 한에폭의 전체값의 평균의 2승?? 2차원 배열로 1에폭?\n",
    "  cost.backward() #최적화하려면 기울기를 그려야해. 딥러닝같으면 앞 단계 손실함수도 전파하는 역할???\n",
    "  optimizer.step() #w와 b를 업데이트\n",
    "\n",
    "  if epoch %10==0:\n",
    "    print(f'Epoch{epoch},Loss:{cost.item()}, w:{W.item()}, b:{b.item()}') #cost.item()텐서속의 숫자를 꺼낸다는 뜻\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc2da5",
   "metadata": {},
   "source": [
    "optimizer.zero_grad()가 필요한 이유는 이전 값이 계속 나와.\n",
    "이전 값을 가지고 계속 미분을 해. 한 에폭마다, 한 베치마다 깨끗히 만들고 작업해."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "16cb3131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "수식을 w로미분한값: 2.0\n",
      "수식을 w로미분한값: 4.0\n",
      "수식을 w로미분한값: 6.0\n",
      "수식을 w로미분한값: 8.0\n",
      "수식을 w로미분한값: 10.0\n",
      "수식을 w로미분한값: 12.0\n",
      "수식을 w로미분한값: 14.0\n",
      "수식을 w로미분한값: 16.0\n",
      "수식을 w로미분한값: 18.0\n",
      "수식을 w로미분한값: 20.0\n",
      "수식을 w로미분한값: 22.0\n",
      "수식을 w로미분한값: 24.0\n",
      "수식을 w로미분한값: 26.0\n",
      "수식을 w로미분한값: 28.0\n",
      "수식을 w로미분한값: 30.0\n",
      "수식을 w로미분한값: 32.0\n",
      "수식을 w로미분한값: 34.0\n",
      "수식을 w로미분한값: 36.0\n",
      "수식을 w로미분한값: 38.0\n",
      "수식을 w로미분한값: 40.0\n",
      "수식을 w로미분한값: 42.0\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "print(w)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  z = 2*w\n",
    "  z.backward()\n",
    "  print('수식을 w로미분한값: {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf0807",
   "metadata": {},
   "source": [
    "torch.manual_seed()를하는 이유는 랜덤 함수 할 때 계속 같은 값이 나와.\n",
    "seed 수는 관례적으로 0을 넣을 때도 있고, 42를 넣을 때도 있어. 어떤 값을 넣어도 상관없고 [고정했다]가 중요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c2533660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 3일 때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 3일 때')\n",
    "for i in range(1,3):\n",
    "    print(torch.rand(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73acf283",
   "metadata": {},
   "source": [
    "자동 미분!!!!!!!!!!!!!!\n",
    "2w**2 + 5 자동 미분하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8a745d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "y = w**2\n",
    "z = 2*y+5\n",
    "\n",
    "z.backward() #기울기 계산\n",
    "\n",
    "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680eeb8",
   "metadata": {},
   "source": [
    "파일을 그대로 불러와서 작업해보자. 판다스 추가!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "365bab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1d2c1763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>73</th>\n",
       "      <th>80</th>\n",
       "      <th>75</th>\n",
       "      <th>152</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>91</td>\n",
       "      <td>90</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73</td>\n",
       "      <td>66</td>\n",
       "      <td>70</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>55</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "      <td>60</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>87</td>\n",
       "      <td>79</td>\n",
       "      <td>90</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>79</td>\n",
       "      <td>70</td>\n",
       "      <td>88</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>70</td>\n",
       "      <td>65</td>\n",
       "      <td>74</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>91</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>73</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>78</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>93</td>\n",
       "      <td>89</td>\n",
       "      <td>96</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>68</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>81</td>\n",
       "      <td>90</td>\n",
       "      <td>93</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>86</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>78</td>\n",
       "      <td>83</td>\n",
       "      <td>77</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>82</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "      <td>89</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>78</td>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>76</td>\n",
       "      <td>83</td>\n",
       "      <td>71</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>96</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    73  80   75  152\n",
       "0   93  88   93  185\n",
       "1   89  91   90  180\n",
       "2   96  98  100  196\n",
       "3   73  66   70  142\n",
       "4   53  46   55  101\n",
       "5   69  74   77  149\n",
       "6   47  56   60  115\n",
       "7   87  79   90  175\n",
       "8   79  70   88  164\n",
       "9   69  70   73  141\n",
       "10  70  65   74  141\n",
       "11  93  95   91  184\n",
       "12  79  80   73  152\n",
       "13  70  73   78  148\n",
       "14  93  89   96  192\n",
       "15  78  75   68  147\n",
       "16  81  90   93  183\n",
       "17  88  92   86  177\n",
       "18  78  83   77  159\n",
       "19  82  86   90  177\n",
       "20  86  82   89  175\n",
       "21  78  83   85  175\n",
       "22  76  83   71  149\n",
       "23  96  93   95  192"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('../data02/data-01-test-score.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e6cb2",
   "metadata": {},
   "source": [
    "df.values는 넘파이 2차원 배열이야.\n",
    "행은 학생(데이터 갯수) 열은 4개이고 (과목 3 + 목표값 1)이야.\n",
    "형태로 따지면 shape(24,4)의 꼴이야.\n",
    "\n",
    "배열로 만들어보자!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ccf388d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 93,  88,  93],\n",
       "       [ 89,  91,  90],\n",
       "       [ 96,  98, 100],\n",
       "       [ 73,  66,  70],\n",
       "       [ 53,  46,  55],\n",
       "       [ 69,  74,  77],\n",
       "       [ 47,  56,  60],\n",
       "       [ 87,  79,  90],\n",
       "       [ 79,  70,  88],\n",
       "       [ 69,  70,  73],\n",
       "       [ 70,  65,  74],\n",
       "       [ 93,  95,  91],\n",
       "       [ 79,  80,  73],\n",
       "       [ 70,  73,  78],\n",
       "       [ 93,  89,  96],\n",
       "       [ 78,  75,  68],\n",
       "       [ 81,  90,  93],\n",
       "       [ 88,  92,  86],\n",
       "       [ 78,  83,  77],\n",
       "       [ 82,  86,  90],\n",
       "       [ 86,  82,  89],\n",
       "       [ 78,  83,  85],\n",
       "       [ 76,  83,  71],\n",
       "       [ 96,  93,  95]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data=df.values[:, :-1]\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3dc53d",
   "metadata": {},
   "source": [
    ":-1 은 마지막 열 빼고 전부라는 뜻이야.\n",
    "형태로 따지면 x_data shape(24,3)의 꼴이야.\n",
    "즉, 입력변수가 3짜리야."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0e7a8",
   "metadata": {},
   "source": [
    "y1 = df.values[:, -1]\n",
    "이 결과는 1차원이 됨.\n",
    "shape(24,) 24짜리 벡터임.\n",
    "넘파이에서는 이런 건 행렬이 아니라 벡터(1D)로 취급이 됨.\n",
    "\n",
    "대괄호 2개사용: 여기서 [-1]이 리스트라서 넘파이가 열을 1개 선택하되, 2차원 형태를 유지해줘라고 명령\n",
    "shape(24,1) 열이 1개짜리 2차원 행렬 (24,1)이 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d640c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "[ 3  7 11]\n",
      "[[ 3]\n",
      " [ 7]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": [
    "#연습\n",
    "A=np.arange(12).reshape(3,4)\n",
    "print(A)\n",
    "A1=A[:,-1]\n",
    "print(A1) #shape(3,)\n",
    "A2=A[:,[-1]]\n",
    "print(A2) #shape(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ffeee3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[185],\n",
       "       [180],\n",
       "       [196],\n",
       "       [142],\n",
       "       [101],\n",
       "       [149],\n",
       "       [115],\n",
       "       [175],\n",
       "       [164],\n",
       "       [141],\n",
       "       [141],\n",
       "       [184],\n",
       "       [152],\n",
       "       [148],\n",
       "       [192],\n",
       "       [147],\n",
       "       [183],\n",
       "       [177],\n",
       "       [159],\n",
       "       [177],\n",
       "       [175],\n",
       "       [175],\n",
       "       [149],\n",
       "       [192]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data=df.values[:, [-1]] #1차원 안되게 [] 했다는데 머선말이지\n",
    "y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ae025",
   "metadata": {},
   "source": [
    "텐서로 바꿔보자.\n",
    "근데 이걸 왜 텐서로 바꾸는지 잘 모르겠어.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "41c51d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 93,  88,  93],\n",
      "        [ 89,  91,  90],\n",
      "        [ 96,  98, 100],\n",
      "        [ 73,  66,  70],\n",
      "        [ 53,  46,  55],\n",
      "        [ 69,  74,  77],\n",
      "        [ 47,  56,  60],\n",
      "        [ 87,  79,  90],\n",
      "        [ 79,  70,  88],\n",
      "        [ 69,  70,  73],\n",
      "        [ 70,  65,  74],\n",
      "        [ 93,  95,  91],\n",
      "        [ 79,  80,  73],\n",
      "        [ 70,  73,  78],\n",
      "        [ 93,  89,  96],\n",
      "        [ 78,  75,  68],\n",
      "        [ 81,  90,  93],\n",
      "        [ 88,  92,  86],\n",
      "        [ 78,  83,  77],\n",
      "        [ 82,  86,  90],\n",
      "        [ 86,  82,  89],\n",
      "        [ 78,  83,  85],\n",
      "        [ 76,  83,  71],\n",
      "        [ 96,  93,  95]])\n",
      "tensor([[185],\n",
      "        [180],\n",
      "        [196],\n",
      "        [142],\n",
      "        [101],\n",
      "        [149],\n",
      "        [115],\n",
      "        [175],\n",
      "        [164],\n",
      "        [141],\n",
      "        [141],\n",
      "        [184],\n",
      "        [152],\n",
      "        [148],\n",
      "        [192],\n",
      "        [147],\n",
      "        [183],\n",
      "        [177],\n",
      "        [159],\n",
      "        [177],\n",
      "        [175],\n",
      "        [175],\n",
      "        [149],\n",
      "        [192]])\n"
     ]
    }
   ],
   "source": [
    "xdata=torch.from_numpy(x_data)\n",
    "ydata=torch.from_numpy(y_data)\n",
    "print(xdata)\n",
    "print(ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeed00e",
   "metadata": {},
   "source": [
    "정수로 되어있으니까 float 실수로 바꿔보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a922c8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 93.,  88.,  93.],\n",
      "        [ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.],\n",
      "        [ 53.,  46.,  55.],\n",
      "        [ 69.,  74.,  77.],\n",
      "        [ 47.,  56.,  60.],\n",
      "        [ 87.,  79.,  90.],\n",
      "        [ 79.,  70.,  88.],\n",
      "        [ 69.,  70.,  73.],\n",
      "        [ 70.,  65.,  74.],\n",
      "        [ 93.,  95.,  91.],\n",
      "        [ 79.,  80.,  73.],\n",
      "        [ 70.,  73.,  78.],\n",
      "        [ 93.,  89.,  96.],\n",
      "        [ 78.,  75.,  68.],\n",
      "        [ 81.,  90.,  93.],\n",
      "        [ 88.,  92.,  86.],\n",
      "        [ 78.,  83.,  77.],\n",
      "        [ 82.,  86.,  90.],\n",
      "        [ 86.,  82.,  89.],\n",
      "        [ 78.,  83.,  85.],\n",
      "        [ 76.,  83.,  71.],\n",
      "        [ 96.,  93.,  95.]])\n",
      "tensor([[185.],\n",
      "        [180.],\n",
      "        [196.],\n",
      "        [142.],\n",
      "        [101.],\n",
      "        [149.],\n",
      "        [115.],\n",
      "        [175.],\n",
      "        [164.],\n",
      "        [141.],\n",
      "        [141.],\n",
      "        [184.],\n",
      "        [152.],\n",
      "        [148.],\n",
      "        [192.],\n",
      "        [147.],\n",
      "        [183.],\n",
      "        [177.],\n",
      "        [159.],\n",
      "        [177.],\n",
      "        [175.],\n",
      "        [175.],\n",
      "        [149.],\n",
      "        [192.]])\n"
     ]
    }
   ],
   "source": [
    "xdata=torch.from_numpy(x_data).float()\n",
    "ydata=torch.from_numpy(y_data).float()\n",
    "print(xdata)\n",
    "print(ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1841e7a0",
   "metadata": {},
   "source": [
    "w를 3행 1열로 만들어야해."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "32972a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W=torch.zeros((3,1), requires_grad=True)\n",
    "b=torch.zeros(1, requires_grad=True)\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9bfd2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.SGD([W,b], lr=0.0001) #10**-4 정도\n",
    "epochs=20001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3513ea5",
   "metadata": {},
   "source": [
    "loss 값이 많이 안줄면 멈춰도 된다. 더 할 필요는 없어."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4220e746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:26966.458984375, W:tensor([[2.6317],\n",
      "        [2.6377],\n",
      "        [2.7106]], requires_grad=True), b:tensor([0.0325], requires_grad=True)\n",
      "Epoch:100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:1000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:1100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:1200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:1300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:1400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:1500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:1600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:1700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:1800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:1900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:2000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:2100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:2200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:2300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:2400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:2500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:2600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:2700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:2800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:2900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:3000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:3100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:3200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:3300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:3400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:3500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:3600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:3700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:3800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:3900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:4000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:4100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:4200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:4300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:4400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:4500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:4600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:4700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:4800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:4900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:5000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:5100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:5200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:5300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:5400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:5500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:5600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:5700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:5800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:5900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:6000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:6100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:6200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:6300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:6400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:6500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:6600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:6700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:6800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:6900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:7000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:7100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:7200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:7300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:7400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:7500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:7600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:7700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:7800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:7900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:8000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:8100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:8200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:8300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:8400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:8500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:8600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:8700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:8800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:8900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:9000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:9100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:9200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:9300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:9400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:9500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:9600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:9700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:9800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:9900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:10000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:10100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:10200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:10300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:10400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:10500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:10600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:10700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:10800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:10900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:11000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:11100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:11200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:11300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:11400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:11500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:11600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:11700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:11800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:11900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:12000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:12100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:12200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:12300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:12400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:12500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:12600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:12700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:12800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:12900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:13000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:13100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:13200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:13300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:13400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:13500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:13600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:13700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:13800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:13900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:14000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:14100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:14200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:14300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:14400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:14500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:14600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:14700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:14800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:14900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:15000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:15100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:15200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:15300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:15400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:15500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:15600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:15700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:15800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:15900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:16000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:16100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:16200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:16300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:16400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:16500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:16600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:16700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:16800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:16900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:17000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:17100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:17200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:17300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:17400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:17500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:17600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:17700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:17800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:17900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:18000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:18100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:18200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:18300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:18400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:18500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:18600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:18700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:18800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:18900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:19000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:19100, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:19200, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:19300, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:19400, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:19500, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:19600, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:19700, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:19800, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:19900, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n",
      "Epoch:20000, Loss:nan, W:tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True), b:tensor([nan], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "  optimizer.zero_grad()\n",
    "  y=xdata.matmul(W)+b #배열의 곱이라서 matmul 사용을 해야해 아니면 dot 하던가!!!!!!!!!!!\n",
    "  cost=torch.mean((y-ydata)**2)\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  if epoch %100 ==0:\n",
    "    print(f'Epoch:{epoch}, Loss:{cost}, W:{W}, b:{b}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c9d75",
   "metadata": {},
   "source": [
    "nn.Module로 구현하는 선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e0fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f91f673",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43bd0ca2",
   "metadata": {},
   "source": [
    "클래스로 파이토치 모델 구현!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5d6a7b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[185.],\n",
       "        [180.],\n",
       "        [196.],\n",
       "        [142.],\n",
       "        [101.],\n",
       "        [149.],\n",
       "        [115.],\n",
       "        [175.],\n",
       "        [164.],\n",
       "        [141.],\n",
       "        [141.],\n",
       "        [184.],\n",
       "        [152.],\n",
       "        [148.],\n",
       "        [192.],\n",
       "        [147.],\n",
       "        [183.],\n",
       "        [177.],\n",
       "        [159.],\n",
       "        [177.],\n",
       "        [175.],\n",
       "        [175.],\n",
       "        [149.],\n",
       "        [192.]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data\n",
    "y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f7958b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "  def __inti__(self):\n",
    "    super().__init__() #파이썬은 init이 생성자고, super class를 생성한다는 말???? ㅇㅁㅇ??? 부른다는 말????\n",
    "    self.linear=nn.Linear(3,1)\n",
    "#모델 완성\n",
    "\n",
    "#학습할때 부르는 함수 생성\n",
    "  def forward(self, x):\n",
    "    return self.linear(x) #x데이터를 넣으면 이 함수를 호출함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "25ff6ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7e35d5c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m      2\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50001\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\it\\anaconda3\\Lib\\site-packages\\torch\\optim\\sgd.py:65\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[1;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable, fused)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov \u001b[38;5;129;01mand\u001b[39;00m (momentum \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dampening \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNesterov momentum requires a momentum and zero dampening\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(params, defaults)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_supports_amp_scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\it\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:396\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    394\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 396\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    398\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-6)\n",
    "epochs=50001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  y=model(x_data) #y와 b 값을 가진 model에 의해 나온 y값\n",
    "  cost=F.mse_loss(y,y_data)\n",
    "  optimizer.zero_grad() #for 문 돌면서 초기화\n",
    "\n",
    "  cost.backward() #기울기 계산\n",
    "  optimizer.step() #wb를 업데이트\n",
    "\n",
    "  if epoch%100 ==0:\n",
    "    print(f'epoch:{epoch}, cost:{cost.item}, w, b:{list(model.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1941855",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=torch.FloatTensor([[80,90,75]])\n",
    "pred_y=model(new_data)\n",
    "print(pred_y)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849eb1a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "96b88808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0e206a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anagrams',\n",
       " 'anscombe',\n",
       " 'attention',\n",
       " 'brain_networks',\n",
       " 'car_crashes',\n",
       " 'diamonds',\n",
       " 'dots',\n",
       " 'dowjones',\n",
       " 'exercise',\n",
       " 'flights',\n",
       " 'fmri',\n",
       " 'geyser',\n",
       " 'glue',\n",
       " 'healthexp',\n",
       " 'iris',\n",
       " 'mpg',\n",
       " 'penguins',\n",
       " 'planets',\n",
       " 'seaice',\n",
       " 'taxis',\n",
       " 'tips',\n",
       " 'titanic']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.get_dataset_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "74f408c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup</th>\n",
       "      <th>dropoff</th>\n",
       "      <th>passengers</th>\n",
       "      <th>distance</th>\n",
       "      <th>fare</th>\n",
       "      <th>tip</th>\n",
       "      <th>tolls</th>\n",
       "      <th>total</th>\n",
       "      <th>color</th>\n",
       "      <th>payment</th>\n",
       "      <th>pickup_zone</th>\n",
       "      <th>dropoff_zone</th>\n",
       "      <th>pickup_borough</th>\n",
       "      <th>dropoff_borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-23 20:21:09</td>\n",
       "      <td>2019-03-23 20:27:24</td>\n",
       "      <td>1</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.95</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Lenox Hill West</td>\n",
       "      <td>UN/Turtle Bay South</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-04 16:11:55</td>\n",
       "      <td>2019-03-04 16:19:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.30</td>\n",
       "      <td>yellow</td>\n",
       "      <td>cash</td>\n",
       "      <td>Upper West Side South</td>\n",
       "      <td>Upper West Side South</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-27 17:53:01</td>\n",
       "      <td>2019-03-27 18:00:25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.37</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.16</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>West Village</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-03-10 01:23:59</td>\n",
       "      <td>2019-03-10 01:49:51</td>\n",
       "      <td>1</td>\n",
       "      <td>7.70</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.95</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Hudson Sq</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-03-30 13:27:42</td>\n",
       "      <td>2019-03-30 13:37:14</td>\n",
       "      <td>3</td>\n",
       "      <td>2.16</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.40</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Midtown East</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6428</th>\n",
       "      <td>2019-03-31 09:51:53</td>\n",
       "      <td>2019-03-31 09:55:27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.36</td>\n",
       "      <td>green</td>\n",
       "      <td>credit card</td>\n",
       "      <td>East Harlem North</td>\n",
       "      <td>Central Harlem North</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6429</th>\n",
       "      <td>2019-03-31 17:38:00</td>\n",
       "      <td>2019-03-31 18:34:23</td>\n",
       "      <td>1</td>\n",
       "      <td>18.74</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.80</td>\n",
       "      <td>green</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>East Concourse/Concourse Village</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Bronx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6430</th>\n",
       "      <td>2019-03-23 22:55:18</td>\n",
       "      <td>2019-03-23 23:14:25</td>\n",
       "      <td>1</td>\n",
       "      <td>4.14</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.30</td>\n",
       "      <td>green</td>\n",
       "      <td>cash</td>\n",
       "      <td>Crown Heights North</td>\n",
       "      <td>Bushwick North</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6431</th>\n",
       "      <td>2019-03-04 10:09:25</td>\n",
       "      <td>2019-03-04 10:14:29</td>\n",
       "      <td>1</td>\n",
       "      <td>1.12</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>green</td>\n",
       "      <td>credit card</td>\n",
       "      <td>East New York</td>\n",
       "      <td>East Flatbush/Remsen Village</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6432</th>\n",
       "      <td>2019-03-13 19:31:22</td>\n",
       "      <td>2019-03-13 19:48:02</td>\n",
       "      <td>1</td>\n",
       "      <td>3.85</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.16</td>\n",
       "      <td>green</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Boerum Hill</td>\n",
       "      <td>Windsor Terrace</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6433 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pickup             dropoff  passengers  distance  fare  \\\n",
       "0    2019-03-23 20:21:09 2019-03-23 20:27:24           1      1.60   7.0   \n",
       "1    2019-03-04 16:11:55 2019-03-04 16:19:00           1      0.79   5.0   \n",
       "2    2019-03-27 17:53:01 2019-03-27 18:00:25           1      1.37   7.5   \n",
       "3    2019-03-10 01:23:59 2019-03-10 01:49:51           1      7.70  27.0   \n",
       "4    2019-03-30 13:27:42 2019-03-30 13:37:14           3      2.16   9.0   \n",
       "...                  ...                 ...         ...       ...   ...   \n",
       "6428 2019-03-31 09:51:53 2019-03-31 09:55:27           1      0.75   4.5   \n",
       "6429 2019-03-31 17:38:00 2019-03-31 18:34:23           1     18.74  58.0   \n",
       "6430 2019-03-23 22:55:18 2019-03-23 23:14:25           1      4.14  16.0   \n",
       "6431 2019-03-04 10:09:25 2019-03-04 10:14:29           1      1.12   6.0   \n",
       "6432 2019-03-13 19:31:22 2019-03-13 19:48:02           1      3.85  15.0   \n",
       "\n",
       "       tip  tolls  total   color      payment            pickup_zone  \\\n",
       "0     2.15    0.0  12.95  yellow  credit card        Lenox Hill West   \n",
       "1     0.00    0.0   9.30  yellow         cash  Upper West Side South   \n",
       "2     2.36    0.0  14.16  yellow  credit card          Alphabet City   \n",
       "3     6.15    0.0  36.95  yellow  credit card              Hudson Sq   \n",
       "4     1.10    0.0  13.40  yellow  credit card           Midtown East   \n",
       "...    ...    ...    ...     ...          ...                    ...   \n",
       "6428  1.06    0.0   6.36   green  credit card      East Harlem North   \n",
       "6429  0.00    0.0  58.80   green  credit card                Jamaica   \n",
       "6430  0.00    0.0  17.30   green         cash    Crown Heights North   \n",
       "6431  0.00    0.0   6.80   green  credit card          East New York   \n",
       "6432  3.36    0.0  20.16   green  credit card            Boerum Hill   \n",
       "\n",
       "                          dropoff_zone pickup_borough dropoff_borough  \n",
       "0                  UN/Turtle Bay South      Manhattan       Manhattan  \n",
       "1                Upper West Side South      Manhattan       Manhattan  \n",
       "2                         West Village      Manhattan       Manhattan  \n",
       "3                       Yorkville West      Manhattan       Manhattan  \n",
       "4                       Yorkville West      Manhattan       Manhattan  \n",
       "...                                ...            ...             ...  \n",
       "6428              Central Harlem North      Manhattan       Manhattan  \n",
       "6429  East Concourse/Concourse Village         Queens           Bronx  \n",
       "6430                    Bushwick North       Brooklyn        Brooklyn  \n",
       "6431      East Flatbush/Remsen Village       Brooklyn        Brooklyn  \n",
       "6432                   Windsor Terrace       Brooklyn        Brooklyn  \n",
       "\n",
       "[6433 rows x 14 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxis=sns.load_dataset('taxis')\n",
    "taxis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
